---
title: "Final Exam  Intro. to Computational Statistics"
author: "Mohsen Nabian"
date: "8/20/2015"
output: word_document
---

Final Exam - Intro. to Computational Statistics

Unless otherwise specified, assume all alpha (p-value) thresholds to be 0.05, and all tests to be two-sided if that
is an option. All calculations may be done with R or by hand unless otherwise specified.
Please show and explain your work as much as possible, using latex for displaying all math.
Note that all problems are worth 3 points except problem 1, which is worth 7 points, and problems 7(a), 8(a),
and 9(a), which are each worth 5 points.
Good luck!






1. You roll five six-sided dice. Write a script in R to calculate the probability of getting between 15 and 20 (inclusive) as the total amount of your roll (ie, the sum when you add up what is showing on all five dice). 
Exact solutions are preferable but approximate solutions are ok as long as they are precise.

Exact solution:

```{r}
prob<-function(m,sum)
{
  
  if(m==1)
  {
    if((sum>=0)&(sum<=6))
    {return(1/6)}
    if((sum<0)|(sum>6))
    {return(0)}
    
  }
  return(1/6*(prob(m-1,sum-6)+prob(m-1,sum-5)+prob(m-1,sum-4)+prob(m-1,sum-3)+prob(m-1,sum-2)+
                prob(m-1,sum-1)));
  
  
}

P=prob(5,15)+prob(5,16)+prob(5,17)+prob(5,18)+prob(5,19)+prob(5,20)
P
```


Numerical Solution:
```{r}
n=10000
s<-0;
for (i in 1:n)
{
   x1<-sample(1:6, 1)
   x2<-sample(1:6, 1)
   x3<-sample(1:6, 1)
   x4<-sample(1:6, 1)
   x5<-sample(1:6, 1)
   
   s[i]<-x1+x2+x3+x4+x5;
}
P<-sum((s>14)&(s<21))/length(s)
P
```

The results are very close and by increasing n, numerical results would converge to the exact solution.

----------------


2. Create a simulated dataset of 100 observations, where x is a random normal variable with mean 0 and standard deviation 1, and y = 0.1 + 2*x + e, where epsilon is also a random normal error with mean 0 and sd 1.
(One reminder: remember that in creating simulated data with, say, 100 observations, you
need to use rnorm(100) for epsilon, not rnorm(1), to ensure that each observation gets a different
error.)
```{r}
set.seed(1)
x<-rnorm(n=100,mean=0,sd=1)
e<-rnorm(n=100,mean=0,sd=1)
y<-0.1+2*x+e
```
a. Perform a t test for whether the mean of Y equals the mean of X using R.

we consider x and y as two independent variables we want to see if they have the same minimum.
```{r}
df<-data.frame(x=as.numeric(x),y=as.numeric(y))
#df<-as.numeric(df)
t.test(df$y,df$x)
```
with p-value=0.4432 the hypothesis that y and x has the same average is NOT rejected. 


b. Now perform this test by hand using just the first 5 observations. Please write out all your steps in latex.

$$mean_x=\frac{x1+x2+x3+x4+x5}{5}$$


$$se=sd/\sqrt {n} $$
$$se_{diff} = \sqrt{se_{1}^{2} + se_{2}^{2}}$$
$$df = \frac{se_{ab}^{2}}{se_{a}^{4}/(n_{a}-1) + se_{b}^{4}/(n_{b}-1) }$$

```{r}
n<-5
x1<-x[1:n]
y1<-y[1:n]
mean(x1)
sd(x1)
se_x1<-sd(x1)/sqrt(n)
se_x1
mean(y1)
sd(y1)
se_y1<-sd(y1)/sqrt(n)
se_y1
se_diff<-sqrt(se_x1^2+se_y1^2)
se_diff
df_eq<- ((se_diff)^2)/((se_x1)^4/(n-1)+(se_y1)^4/(n-1))
T_statistics=((mean(x1)-mean(y1))/se_diff)
T_statistics
thresholds<-qt(p=0.975,df=df_eq)
thresholds
```

since the t_statistics lies withing the thresholds, the Null hypothesis(True Mean(y)=True Mean(x)) is NOT rejected.


c. Using R, test whether the mean of Y is significantly different from 0.
```{r}
t.test(y,mu=0)

```
So according to the p-value mean(y)=0 is Not rejected. 




d. Again using the first five obsevations, test by hand whether the mean of Y is different from 0.

```{r}
n=5
y1<-y[1:n]
mean(y1)
sd(y1)
se_y<-sd(y1)/sqrt(n)
T_statistics<-mean(y1)/se_y
T_statistics
thresholds<-qt(p=0.975,df=n-1)
thresholds
```
since the t_statistics lies withing the thresholds, the Null hypothesis(True Mean(y)=0) is NOT rejected.



e. Assuming the mean and sd of Y that you calculate from the first five observations would not change, what is the minimum total number of observations you would need to be able to conclude that the mean of Y is different from 0 at the p = 0.01 confidence level?

$$T_{statistics}=\frac{\sqrt{n}\times (\bar{x}-\mu)}{sd}$$
$$Thresholds(p_{value}=0.95)=\pm T_{distribution}(0.975,n-1)$$

By increseing n we increase T_{statistics} to exceed the thresholds to reject the Null-Hypothesis.
Knowing that increasing n would change thresholds very slightly, we may  approximate n as follows and then check if our assumption works.In fact we need to do itteration over n:
n_{old}=5
$$n=(\frac{T_{distribution}(0.995,n_{old}-1)\times sd} {\bar{x}-\mu})^2$$
```{r}
T<-qt(0.995,4)
print(T)
mean(y1)
sd(y1)

```

$$n=(\frac{4.604\times 1.355} {1.750459})^2=12.70$$

So n=13 would be a good choice. Now we substitute df=n-1=12 into the T_distribution.

$$n=(\frac{T_{distribution}(0.995,n-1)\times sd} {\bar{x}-\mu})^2$$
```{r}
T<-qt(0.995,12)
print(T)
mean(y1)
sd(y1)

```

$$n=(\frac{3.05454\times 1.355476} { 1.750459})^2=5.6$$

so it did not converge at all. That means even if we increase n to 10^6 still we are unable to 
reject the Null Hypothesis (True Mean(y)=0).

f. Verify (d) (approximately) by increasing the simulated data to the n you calculated in (e) that would be necessary. If the test of Y = 0 is still not significant, explain why. (Go back to using the original 100-observation dataset for g and h.)

as calculated in e there would be no convergance to define n.
as we increase the n  we are just increasing the random numbers around zero and there would be no specific tendecy or direction of the data to be bossted. 
here we put n=50 to show it:
```{r}
n=50
y2<-y[1:n]
t.test(y2,mu=0)

```

g. Create a categorical (factor) variable c, where c = 1 if x < -1, c = 3 if x > 1, and c = 2 otherwise. Use R to perform an F test for whether the mean of y differs across these three groups.

```{r}
c=0;


for (i in 1:length(x))
{
  if(x[i]<(-1))
  {c[i]=1}
  if(x[i]>1)
  {c[i]=3}
  if((x[i]>=-1)&(x[i]<=1))
  {c[i]=2}
}
c<-as.numeric(c)
df<-data.frame(c=as.numeric(c),x=as.numeric(x))
aov.ex1 = aov(x~c,data=df) 
aov.ex1
summary(aov.ex1)
```
So the TRUE mean for these 3 groups are significantly different.

h. Using the first three observations for each group, calculate the same F test by hand.


$$\frac{\textrm{average variance between groups}}{\textrm{average variance within groups}} $$

$$\textrm{Between variance } = \frac{n_{1}(\bar{y}_{1} - \bar{y})^{2}+ ... + n_{G}(\bar{y}_{G} - \bar{y})^{2} }{G-1} $$

$$\textrm{Within variance } = \frac{(n_{1}-1)s_{1}^{2}+ ... + (n_{G}-1)s_{G}^{2} }{N-G} $$


```{r}
c<-as.numeric(c)
x1<-x[c==1]
x2<-x[c==2]
x3<-x[c==3]

n1<-3
mean1<-mean(x1[1:n1])
sd1<-sd(x1[1:n1])

n2<-3
mean2<-mean(x2[1:n2])
sd2<-sd(x2[1:n2])

n3<-3
mean3<-mean(x3[1:n3])
sd3<-sd(x3[1:n3])

N<-9
G=3;
meanx<-sum(n1*mean1+n2*mean2+n3*mean3)/(n1+n2+n3)

avbg<-(n1*(mean1-meanx)^2+n2*(mean2-meanx)^2+n3*(mean3-meanx)^2)/(G-1)
avwg<-((n1-1)*sd1^2+(n2-1)*sd2^2+(n3-1)*sd3^2)/(N-G)
df1<-G-1;
df2<-N-G
F_stat<-avbg/avwg
F_stat
threshold<-qf(p=0.95,df1,df2,lower.tail = F)
threshold
```
As it is seen, the F-statistics which is way more than the thresholds, thus the Null hypothesis is rejected.
which means the x1,x2 and x3 have not the same true mean.
---------

3. Generate a new 100-observation dataset as before, except now y = 0.1 + 0.2 * x + e
a. Regress y on x using R, and report the results.

```{r}
set.seed(1)
x <- rnorm(100,0,1)
y <- 0.1 + 0.2*x + rnorm(100,0,1)
dat <- data.frame(x=x,y=y)
#To estimate our model - ie, to regress Y on X - we simply run:
biv_model <- lm(y~x,data=dat)
summary(biv_model)

```

b. Discuss the coefficient on x and its standard error, and present the 95% CI.
  
  So the calculated x coefficient is 0.19894 with std.error=0.10773
```{r}
q<-qt(0.975,98)

```
$$B1-se*qt<TRUE mean of B1 < B1+se *qt$$
```{r}

B1<-as.numeric(biv_model$coefficients[2])
se<-0.10773
B1-se*q
B1+se*q
```


so:    -0.0148197<TRUE mean of B1 <0.4126997 with 95% CI


c. Use R to calculate the p-value on the coefficient on x from the t value for that coefficient. What does this p-value represent (be very precise in your language here)?

```{r}
t<-1.847

P_value<-2*(1-pt(t,98))
P_value
```
In fact the p-value that the regression reports is the areat of the two tails.(Pr(abs(t)>))
However, P-value is also (as I remembered) was calculated based on the area of one end tail which is the half of what we calculated here.
```{r}

P_value<-(1-pt(t,98))
P_value

```



d. Discuss the F-statistic and its p-value, and calculate that p-value from the F statistic using R. What does this test and its p-value indicate?


F statistics measures the overall significance of the model.
The Null hypothesis for the F-test is that are all coefficients true mean =0. 


```{r}
P_value<-1-pf(3.41,1,98)
P_value
```
Since p-value is 0.0678 which is close to 0.05, we might say for at least more than 93% chance , The Null is rejected and the overal regression results are significant.


e. Using the first five observations, calculate by hand the coefficient on x, its standard error, and the adjusted R2. Be sure to show your work.


```{r}
y1<-y[1:5]
y1
x1<-x[1:5]
x1
mean(x1)
sd(x1)
mean(y1)
sd(y1)
var(y1)
cov(x1,y1)

```
$$\bar{x}=0.1293$$
$$s_x=0.9610$$
$$\bar{y}=-0.2713$$
$$s_y=0.6342$$

$$\textrm{Var}(x) = \frac{1}{n-1}\sum_{i} (x_{i} - \bar{x})^{2}$$
$$\textrm{Var}(x)=\frac{1}{5-1}((-0.626-0.1293)^2+(0.1836-0.1293)^2+(-0.8356-0.1293)^2+(1.5952-0.1293)^2+(0.3295-0.1293)^2)=0.92335$$
$$\textrm{Var}(y) = \frac{1}{n-1}\sum_{i} (y_{i} - \bar{y})^{2}$$
$$\textrm{Var}(y)=\frac{1}{5-1}((-0.6456+0.2713)^2+(0.1788+0.2713)^2+(-0.9780+0.2713)^2+( 0.5770+0.2713)^2+(-0.4886+0.2713)^2)=0.4022$$
$$\textrm{Cov}(x,y) = \frac{1}{(n-1)} \sum_{i} (x_{i} - \bar{x})(y_{i} - \bar{y})$$
$$\textrm{Cov}(x,y)=\frac{1}{(5-1)}\times((-0.6264538-0.1293)(-0.6456574+0.2713)+(0.1836433-0.1293)(0.178844+0.2713)+(-0.8356286-0.1293)(-0.97804+0.2713)+(1.5952808-0.1293)(0.57708+0.2713)+(0.3295078-0.1293)(-0.4886+0.2713))= 0.547385$$


$$y = \beta_{0} + \beta_{1}x$$
$$\beta_{1} = \frac{\textrm{Cov}(x,y)}{\textrm{Var}(x)} =\frac{0.547385}{0.9234}=0.5928

$$\beta_{0} = \bar{y} - \beta_{1} \bar{x}=-0.2713-0.5928\times 0.129=-0.3477$$

$$y = \beta_{0} + \beta_{1}x$$
$$y=-0.3477+0.5928\times x$$

$$x(1)=-0.6264 ,\hat{Y(1)}=-0.3477+0.5928*(-0.6456574)=-0.7304457$$
$$x(2)=0.1836 ,\hat{Y(2)}=-0.3477+0.5928*0.1788445= -0.241681$$
$$x(3)=-0.8356 ,\hat{Y(3)}=-0.3477+0.5928*(-0.9780474)=-0.9274865$$
$$x(4)=1.5952 ,\hat{Y(4)}=-0.3477+0.5928* (0.5770849)=-0.005604071$$
$$x(5)=0.32950 ,\hat{Y(5)}=-0.3477+0.5928* (-0.4886831)=-0.6373913$$

$$se_{\hat{y}} = \sqrt{ \frac{\sum (y_i-\hat{y}_i)^2 }{n-2}}$$

$$se_{\hat{y}} = \sqrt{ \frac{ (-0.7304457+0.6456574)^2+(-0.241681-0.1788445)^2+(-0.9274865+0.9780474)^2+(-0.005604071-0.5770849)^2 +(-0.6373913+0.4886831)}{5-2}}=0.09435135$
$$se_{\beta_0} = se_{\hat{y}} \sqrt{ \frac{\sum x_i^2}{n \sum (x_i - \bar{x})^2}}$$
$$se_{\beta_1} = se_{\hat{y}} \frac{1}{\sqrt{\sum (x_i - \bar{x})^2}}$$
$$se_{\beta_1} =0.09435135\times \frac{1}{\sqrt{(-0.626-0.1293)^2+(0.1836-0.1293)^2+(-0.8356-0.1293)^2+(1.5952-0.1293)^2+(0.3295-0.1293)^2}}=0.04909474$$

$$T_Statistics=\frac{B1-0}{se_{\beta_1}}$$
$$T_Statistics=\frac{0.5928-0}/0.04909474=12.0748$$
$$dg_freedom=n-k-1=5-1-1=3$$


$$TSS = \sum_{i} (y_{i} - \bar{y})^{2}=(-0.6456+0.2713)^2+(0.1788+0.2713)^2+(-0.9780+0.2713)^2+
(0.5770+0.2713)^2+(-0.4886+0.2713)^2=1.608948$$
$$SSE = \sum_{i} (y_{i} - \hat{y}_{i})^{2}=(-0.7304457+0.6456574)^2+(-0.241681-0.1788445)^2+(-0.9274865+0.9780474)^2+(-0.005604071-0.5770849)^2 +(-0.6373913+0.4886831)=0.3774054$$
$$df_t=n-1=4$$
$$df_e=n-k-1=3$$
$$adjusted R^2=\frac{TSS/df_t-SSE/df_e}{TSS/df_t}$$
$$adjusted R^2=(1.6089/4-0.3774/3)/(1.608948/4)=0.6872


 
4. Now generate y = 0.1 + 0.2 * x - 0.5 * x^2 + e with 100 observations.


```{r}
set.seed(1)
x <- rnorm(100,0,1)
y <- 0.1 + 0.2*x -0.5*x^2+ rnorm(100,0,1)
dat <- data.frame(x=x,y=y)

```



a. Regress y on x and x^2 and report the results. If x or x^2 are not statistically significant, suggest why.

```{r}

biv_model <- lm(y~x+I(x^2),data=dat)
summary(biv_model)

```
x and x^2 are both statistically significant.


b. Based on the known coefficients that we used to create y, what is the effect on y of increasing x by 1 unit from 1 to 2?
```{r}

x1<-1
x2<-2
x<-x1
y1 = 0.1 + 0.2 * x - 0.5 * x^2
x<-x2
y2 = 0.1 + 0.2 * x - 0.5 * x^2

print(y2-y1)

```



c. Based on the coefficients estimated from 4(a), what is the effect on y of changing x from -0.5 to -0.7?
```{r}
beta<-as.numeric(biv_model$coefficients)
x<-(-0.5)
y1<-beta[1]+beta[2]*x+beta[3]*x^2

x<-(-0.7)
y2<-beta[1]+beta[2]*x+beta[3]*x^2
print(y2-y1)


```
5. Now generate x2 as a random normal variable with a mean of -1 and a sd of 1. Create a new dataset
where y = 0.1 + 0.2 * x +20.5 ??? x ??? x2 + e.

```{r}
set.seed(100)
x <- rnorm(100,0,1)
x2<- rnorm(100,-1,1)
y <-0.1 + 0.2*x-0.5*x*x2
df<-data.frame(x,x2,y)
```
a. Based on the known coefficients, what is the effect of increasing x2 from 0 to 1 with x held at its mean?

```{r}

a<-mean(x)
b1<-0
b2<-1
y1 <-0.1 + 0.2*a-0.5*a*b1
y2 <-0.1 + 0.2*a-0.5*a*b2
y1
y2
print(y2-y1)

```

b. Regress y on x, x2, and their interaction. Based on the regression-estimated coefficients, what is the
effect on y of shifting x from -0.5 to -0.7 with x2 held at 1?

```{r}

reg1<-lm(y~x+x2+x*x2,data = df)
summary(reg1)
```

The coefficients in the regresion model are 100% equal to the true value and consequently, R^2=1

```{r}
beta<-as.numeric(reg1$coefficients)
yp<-beta[1]+beta[2]*x+beta[3]*x1+beta[4]*x*x1
b<-1
a1<-(-0.5)
a2<-(-0.7)
y1<-beta[1]+beta[2]*a1+beta[3]*b+beta[4]*a1*b
y2<-beta[1]+beta[2]*a2+beta[3]*b+beta[4]*a2*b
print(y2-y1)

```

c. Regress the current y on x alone. Using the R2
from this regression and the R2 from 5(b), perform by hand an F test of the complete model (5b) against the reduced, bivariate model. What does this test tell you?

```{r}
reg2<-lm(y~x,data = df)
summary(reg2)
r2_r<-0.6688        # r squared reduced

r2_c<-1     # r squared complete regression 
df1<-2   #number of additional variables
df2<-(100-3-1)
F_stat<- ((r2_c^2-r2_r^2)/df1)/((1-r2_c^2)/df2)
F_stat
1-pf(F_stat,2,(100-6-1))


```

This very small p-value tells us that the complete model is infact highly boost the model and is much much better. 

6. Generate a new variable y2 using the data from (5) which is 1 if y > 0 and 0 otherwise.
a. Perform a logistic regression of y2 on x, x2, and their interaction, and interpret the results.


```{r}
y2<-ifelse(y>0,1,0)
df<-data.frame(x,x2,y2)
reg1<-glm(y2~x+x2+x*x2,data=df,family="binomial")
summary(reg1)
```

Unfortunately the logistic regression algorithm is not converging. 

b. What is the effect of increasing x2 from 0 to 1 with x held at its mean on the probability that y2 is 1?

$$\frac{P(y=1)}{1-P(y=1)} = e^{\beta_0 + \beta_1 x+\beta_2 x_2+\beta_3 x*x_2}$$


```{r}
mean(x)

```
If we had the coefficients the procedure is very easy and straightforward: 

1) so having x2=0 and x=0.003 we simply calculate p(y=1)=a1
2)having x2=1 and x=0.003 we simply calculate p(y=1)=a2
3) ans=a2-a1

-----------------------

7. Generate a dataset with 300 observations and three variables: f, x1, and x2. f should be a factor with three levels, where level 1 corresponds to observations 1-100, level 2 to 101-200, and level 3 to 201-300.Create x1 and x2 such that the first 100 observations have a mean of 1 for x1 and 1 for x2, each with a standard deviation of 2; the second 100 observations have a mean of 0 for x1 and 1 for x2, both with a standard deviation of 1; and the third 100 observations have a mean of 1 for x1 and 0 for x2, both with  a standard deviation of 0.5.

```{r}

x1<-c(rnorm(100,1,2),rnorm(100,0,1),rnorm(100,1,0.5))
x2<-c(rnorm(100,1,2),rnorm(100,1,1),rnorm(100,0,0.5))
y<-as.factor(c(rep(1,100),rep(2,100),rep(3,100)))
df<-data.frame(x1,x2,y)
head(df)
```



a. Using the k-means algorithm, peform a cluster analysis of these data using a k of 3 (use only x1 and
x2 in your calculations; use f only to verify your results). Comparing your clusters with f, how many
datapoints are correctly classified into the correct cluster? How similar are the centroids from your
analysis to the true centers?
b. Perform a factor analysis of this data using your preferred function. Using the scree plot, how many
factors do you think you should include? Speculate about how these results relate to those you got
with the cluster analysis.
```{r}
set.seed(1)
cat <- as.factor(floor(runif(300,1,4)))
df <- cbind(df,cat)
for(i in 1:100)  #100 iteration
{
  # 2(a): get centroids of two groups
  centroids <- aggregate(df[,1:2],by=list(cat=df$cat),FUN=mean) 
  
  # 2(b): calculate distances of each point to centroid 1 (d1) and centroid 2 (d2)
  d1 <- sqrt( (df[,1]-centroids[1,2])^2 + (df[,2]-centroids[1,3])^2 )
  d2 <- sqrt(  (df[,1]-centroids[2,2])^2 + (df[,2]-centroids[2,3])^2 )
  d3 <- sqrt(  (df[,1]-centroids[3,2])^2 + (df[,2]-centroids[3,3])^2 )
  # then reassign the category variable depending on which centroid is closer 
  for (j in 1:length(df[,1]))
  {
          if ((d1[j]<d2[j])&(d1[j]<d3[j]))
        {
          df$cat[j] <- 1
        }
        if ((d2[j]<d1[j])&(d2[j]<d3[j]))
        {
          df$cat[j]<- 2
        }
         if ((d3[j]<d1[j])&(d3[j]<d2[j]))
        {
          df$cat[j]<- 3
        }
  }
}
sum(df$y==df$cat)
print(centroids)
true_centrid<-rbind(c(1,1),c(0,1),c(1,0))
true_centrid
```

so 176 out of 300 observations are classified correctly. 
So the centroids calculated by the algorithm is not very accuarate but some how close to the true centroids. 
----------------------

8. Generate a dataset of 200 observations, this time with 90 independent variables, each of mean 0 and sd
1. Create y such that:
y = 2x1 + ... + 2x30 ??? x31 ??? ... ??? x60 + 0 * x61 + ... + 0 * x90 + e
where e is a random normal variable with mean 0 and sd 10. (Ie, the first 30 x's have a coefficient
of 2; the next 30 have a coefficient of -1; and the last 30 have a coefficient of 0.)
```{r}
df<-rnorm(200,0,1)
y<-0;
for (i in 1:89)
{
 
    df<-cbind(df,rnorm(200,0,1))
}
for (i in 1:200)
{
 
    y[i]<-2*sum(df[i,1:30])-1*sum(df[i,31:60])
}

df<-as.data.frame(cbind(df,y))

```

a. Perform an elastic net regression of y on all the x variables using just the first 100 observations. Use
10-fold cross-validation to find the best value of lambda and approximately the best value of alpha.
```{r}
insample <- df[1:100,]
outsample <- df[101:200,]
library(glmnet)
trainx<-as.matrix(insample[,1:90])
trainy<-insample[,91]
testx<-as.matrix(outsample[,1:90])
testy<-outsample[,91]

lambdalevels <- 10^seq(7,-2,length=100)


############## 
reg=cv.glmnet(trainx,trainy,alpha=1,lambda=lambdalevels)

bestlambda <- reg$lambda.min
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)



reg=cv.glmnet(trainx,trainy,alpha=0.8,lambda=lambdalevels)
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)
#37.011

reg=cv.glmnet(trainx,trainy,alpha=0.6,lambda=lambdalevels)
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)


reg=cv.glmnet(trainx,trainy,alpha=0.4,lambda=lambdalevels)
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)


reg=cv.glmnet(trainx,trainy,alpha=0.2,lambda=lambdalevels)
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)


reg=cv.glmnet(trainx,trainy,alpha=0,lambda=lambdalevels)
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)

```

So the best results (lowest Mean square error(MSE)) comes with alpha=0.2 .


```{r}
reg=cv.glmnet(trainx,trainy,alpha=0.2,lambda=lambdalevels)

plot(reg)
bestlambda <- reg$lambda.min
print(bestlambda)
mse.min <- reg$cvm[reg$lambda == reg$lambda.min]
print(mse.min)

```



b. How accurate are your coefficients from (a)? Summarize your results any way you like, but please don't
give us the raw coefficients from 90 variables.

```{r}
head(predict(reg, type="coefficients",s=bestlambda))

```

we have a great output. all first 30 coefficients are close to 2, the next 30 close to -1 and the last 30are close to 0. Just as what is expected.  


c. Using the results from (b), predict y for the second 100 observations. How accurate is your prediction?

```{r}

yhat.l <- predict(reg$glmnet.fit, s=reg$lambda.min, newx=testx)
mse.las <- sum((testy - yhat.l)^2)/nrow(testx)
mse.las

```
d. Attempt to compare the predictive accuracy here to the accuracy of a prediction made using regular
multiple regression. Explain your results, including if the regular regression failed for any reason.

```{r}
lmout <- lm(trainy~trainx)
head(summary(lmout))
yhat.r <- cbind(1,testx) %*% lmout$coefficients
# ^^ we predict via matrix multiplication rather than just using predict() 
#    because x was made a matrix to make glmnet happy
mse.reg <- sum((testy - yhat.r)^2)/nrow(testx)
mse.reg
```
what we see here is a perfect fit between regression coefficients and real values in a way that R^2 =1.
This is may be because the data size is very well suited for multiple regression. We do not have too much observation (very high n) to cause over fitting. 
Having very large n (~ 10^6)however, I expect elastic net method more accurate.beacuase of overfitting.

------------

9. As in problem 6, use the data from 8 to generate a new y2 that is 1 if y > 0 and 0 otherwise.
a. Using the same process as in 8, estimate an SVM model of y2 on all the x variables for the first 100
variables. Use 10-fold cross-validation to select the best kernel.
b. Using the results from (a), predict y2 for the second 100 observations, and report your accuracy.


```{r}
library(e1071)
y2<-as.factor(ifelse(y>0,1,0))
df<-cbind(df,y2)
df<-df[,-91]   #removing y
traindf<-df[1:100,]
testdf<-df[101:200,]
costvalues <- 10^seq(-3,2,1)


tuned.svm <- tune(svm,y2~.,data=traindf,ranges=list(cost=costvalues), kernel="linear")
yhat <- predict(tuned.svm$best.model,newdata=testdf)
table(predicted=yhat,truth=testdf$y)
sum(yhat==testdf$y)/length(testdf$y)



tuned.svm <- tune(svm,y2~.,data=traindf,ranges=list(cost=costvalues), kernel="radial")
yhat <- predict(tuned.svm$best.model,newdata=testdf)
table(predicted=yhat,truth=testdf$y)
sum(yhat==testdf$y)/length(testdf$y)

```

apparently linear kernel (81% accurate) works better than radial kernel(70% accurate) in this dataset. 






